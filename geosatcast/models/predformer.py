import math
import torch
from torch import nn, einsum
import torch.nn.functional as F
from einops import rearrange
from einops.layers.torch import Rearrange
from fvcore.nn import FlopCountAnalysis, flop_count_table

################################################################################
# Local replacements for timm.components
################################################################################

def to_2tuple(x):
    """
    Converts an integer or float to a tuple of length two.
    If it's already a tuple/list, returns as-is.
    """
    if isinstance(x, (tuple, list)):
        return x
    return (x, x)

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    """
    Implements truncated normal initialization, taken from PyTorch’s master branch
    (similar to what timm uses). For details see:
    https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Distributions.cpp
    """
    def norm_cdf(x):
        """CDF of the standard normal distribution"""
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        # This is to maintain approximately same output regardless of distribution tail
        raise ValueError("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                         "The distribution of values may be incorrect.")

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and then
        # using the inverse CDF for the normal distribution.
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # uniform_ generates [0, 1), which is 2**-1 intervals
        tensor.uniform_(0, 1)
        tensor.mul_(u - l).add_(l)
        # clamp for numerical stability
        tensor.clamp_(0, 1)
        # inverse cdf transform
        tensor.erfinv_()
        tensor.mul_(math.sqrt(2.0) * std)
        tensor.add_(mean)
        # clamp to ensure it stays in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    """
    Fills the input tensor with values drawn from a truncated normal distribution.
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

class DropPath(nn.Module):
    """
    Stochastic depth (DropPath) per sample (when applied in main path of residual blocks).
    From https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py
    """
    def __init__(self, drop_prob=0.):
        super().__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        if not self.training or self.drop_prob == 0.:
            return x
        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # broadcast over all but the batch dimension
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()  # binarize
        output = x.div(keep_prob) * random_tensor
        return output

################################################################################
# Definitions for the rest of your model (SwiGLU, attention, etc.)
################################################################################

class PreNorm(nn.Module):
    """ Layer norm + function wrapper """
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn

    def forward(self, x):
        return self.fn(self.norm(x))

class FeedForward(nn.Module):
    """ Standard MLP w/ GELU """
    def __init__(self, dim, hidden_dim, dropout=0.):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        return self.net(x)

class Attention(nn.Module):
    """ Standard multi-head self-attention (no timm dependencies) """
    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):
        super().__init__()
        inner_dim = dim_head * heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head ** -0.5

        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)
        self.to_out = (
            nn.Sequential(
                nn.Linear(inner_dim, dim),
                nn.Dropout(dropout)
            )
            if project_out else nn.Identity()
        )

    def forward(self, x):
        b, n, d = x.shape
        h = self.heads
        qkv = self.to_qkv(x).chunk(3, dim=-1)
        q, k, v = (rearrange(t, 'b n (h hd) -> b h n hd', h=h) for t in qkv)

        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale
        attn = dots.softmax(dim=-1)
        out = einsum('b h i j, b h j d -> b h i d', attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)

class SwiGLU(nn.Module):
    """
    Gated linear unit: 
       out = (SiLU(Wg*x + bg)) * (Wx*x + bx)
    with the second projection as a linear activation. Then:
       out = LN(...) -> Linear -> Dropout
    """
    def __init__(
            self,
            in_features,
            hidden_features=None,
            out_features=None,
            act_layer=nn.SiLU,
            norm_layer=None,
            bias=True,
            drop=0.
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        bias = to_2tuple(bias)
        drop_probs = to_2tuple(drop)

        self.fc1_g = nn.Linear(in_features, hidden_features, bias=bias[0])  # "gate" projection
        self.fc1_x = nn.Linear(in_features, hidden_features, bias=bias[0])  # "transform" projection
        self.act = act_layer()
        self.drop1 = nn.Dropout(drop_probs[0])
        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()
        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])
        self.drop2 = nn.Dropout(drop_probs[1])

    def forward(self, x):
        x_gate = self.fc1_g(x)
        x_lin = self.fc1_x(x)
        x_glu = self.act(x_gate) * x_lin
        x_glu = self.drop1(x_glu)
        x_glu = self.norm(x_glu)
        x_glu = self.fc2(x_glu)
        x_glu = self.drop2(x_glu)
        return x_glu

class GatedTransformer(nn.Module):
    """
    Stacks multiple "PreNorm(Attention)" and "PreNorm(SwiGLU)" blocks,
    with optional DropPath for each residual.
    """
    def __init__(self, dim, depth, heads, dim_head, mlp_dim,
                 dropout=0., attn_dropout=0., drop_path=0.1):
        super().__init__()
        self.layers = nn.ModuleList([])
        self.norm = nn.LayerNorm(dim)
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=attn_dropout)),
                PreNorm(dim, SwiGLU(dim, mlp_dim, drop=dropout)),
                DropPath(drop_path) if drop_path > 0. else nn.Identity(),
                DropPath(drop_path) if drop_path > 0. else nn.Identity()
            ]))
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward(self, x):
        for attn, ff, drop_path1, drop_path2 in self.layers:
            # Attention + residual
            x = x + drop_path1(attn(x))
            # MLP + residual
            x = x + drop_path2(ff(x))
        return self.norm(x)

def sinusoidal_embedding(n_channels, dim):
    """
    Generates a 2D sinusoidal positional embedding of shape [1, n_channels, dim].
    """
    pe = torch.FloatTensor([
        [p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]
        for p in range(n_channels)
    ])
    pe[:, 0::2] = torch.sin(pe[:, 0::2])
    pe[:, 1::2] = torch.cos(pe[:, 1::2])
    return rearrange(pe, '... -> 1 ...')  # shape: [1, n_channels, dim]

################################################################################
# The main PredFormer model
################################################################################

class PredFormer_Model(nn.Module):
    def __init__(self, model_config, in_steps, **kwargs):
        super().__init__()
        # self.image_height = model_config['height']
        # self.image_width = model_config['width']
        self.patch_size = model_config['patch_size']
        # self.num_patches_h = self.image_height // self.patch_size
        # self.num_patches_w = self.image_width // self.patch_size
        # self.num_patches = self.num_patches_h * self.num_patches_w
        self.num_frames_in = model_config['pre_seq']

        self.dim = model_config['dim']
        self.in_channels = model_config['in_channels']
        self.out_channels = model_config['out_channels']
        self.heads = model_config['heads']
        self.dim_head = model_config['dim_head']
        self.dropout = model_config['dropout']
        self.attn_dropout = model_config['attn_dropout']
        self.drop_path = model_config['drop_path']
        self.scale_dim = model_config['scale_dim']

        self.Ndepth = model_config['Ndepth']  # "depth" for each GatedTransformer
        self.in_steps = in_steps

        # assert self.image_height % self.patch_size == 0, \
        #     'Image height must be divisible by the patch size.'
        # assert self.image_width % self.patch_size == 0, \
        #     'Image width must be divisible by the patch size.'

        self.in_patch_dim = self.in_channels * (self.patch_size ** 2)
        self.out_patch_dim = self.out_channels * (self.patch_size ** 2)

        # Convert each patch into an embedding of dimension self.dim
        self.to_patch_embedding = nn.Sequential(
            Rearrange('b t c (h p1) (w p2) -> b t (h w) (p1 p2 c)',
                      p1=self.patch_size, p2=self.patch_size),
            nn.Linear(self.in_patch_dim, self.dim)
        )

        # # A (fixed, non-learnable) sinusoidal positional embedding
        # pos_emb = sinusoidal_embedding(
        #     self.num_frames_in * self.num_patches,  # total tokens
        #     self.dim
        # )
        # # shape: [1, total_tokens, dim]
        # # We want it viewed as [1, T, N, dim]
        # self.pos_embedding = nn.Parameter(
        #     pos_emb.view(1, self.num_frames_in, self.num_patches, self.dim),
        #     requires_grad=False
        # )

        # "temporal" transformer first
        self.temporal_transformer = GatedTransformer(
            self.dim, self.Ndepth, self.heads, self.dim_head,
            self.dim * self.scale_dim, self.dropout, self.attn_dropout, self.drop_path
        )
        # "space" transformer second
        self.space_transformer = GatedTransformer(
            self.dim, self.Ndepth, self.heads, self.dim_head,
            self.dim * self.scale_dim, self.dropout, self.attn_dropout, self.drop_path
        )

        # Final layer to map each token back to the patch dimension
        self.mlp_head = nn.Sequential(
            nn.LayerNorm(self.dim),
            nn.Linear(self.dim, self.out_patch_dim)  # maps dimension -> patch_dim
        )

    def single_step_forward(self, x):
        """
        x shape: [B, T, C, H, W]
        Returns predicted frames with same shape [B, T, C, H, W].
        """
        B, T, C, H, W = x.shape
        N_h = H // self.patch_size
        N_w = W // self.patch_size
        N = N_h * N_w   # number of patches

        # 1) Patch embedding as usual
        x = self.to_patch_embedding(x)  # => shape [B, T, N, dim]

        # 2) Create a sinusoidal embedding of shape [1, T*N, dim]
        #    then reshape to [1, T, N, dim].
        #    No 'nn.Parameter' here, so it’s not fixed to a single shape
        pos_emb = sinusoidal_embedding(T * N, self.dim)  # => shape [1, T*N, dim]
        pos_emb = pos_emb.view(1, T, N, self.dim).to(x.device)

        # 3) Add the position embedding
        x = x + pos_emb

        b, t, n, d = x.shape
        x_t = rearrange(x, 'b t n d -> (b n) t d')
        x_t = self.temporal_transformer(x_t)
        x_ts = rearrange(x_t, '(b n) t d -> b n t d', b=b)
        x_ts = rearrange(x_ts, 'b n t d -> b t n d')
        x_ts = rearrange(x_ts, 'b t n d -> (b t) n d')
        x_ts = self.space_transformer(x_ts)
        x_ts = x_ts.reshape(-1, d)
        x_out = self.mlp_head(x_ts)
        x_out = x_out.view(B, T, N, self.out_channels, self.patch_size, self.patch_size)
        x_out = rearrange(
            x_out,
            'b t (h w) c ph pw -> b t c (h ph) (w pw)',
            h=N_h,
            w=N_w
        )
        return x_out

    def forward(self, x: torch.Tensor, inv: torch.Tensor, n_steps=1) -> torch.Tensor:
        """
        Autoregressive loop, combining x + the next chunk of 'inv' at each step.
        x:   [B, T_in, C_in, H, W]   (the "history" frames)
        inv: [B, T_inv, C_inv, H, W] (the extra data or 'inv' frames)
        n_steps: how many frames we want to predict forward.

        We'll produce [B, n_steps, outC, H, W].
        """
        x = x.permute(0,2,1,3,4)
        inv = inv.permute(0,2,1,3,4)
        B, T_in, C_in, H, W = x.shape
        preds = []
        for i in range(n_steps):
            step_inv = inv[:, i : i + self.in_steps]
            z = torch.cat([x, step_inv], dim=2)
            next_frame = self.single_step_forward(z)
            preds.append(next_frame[:, -1:])
            x = torch.cat([x[:, 1:], next_frame[:, -1:]], dim=1)

        # after the loop, stack predictions:
        preds = torch.cat(preds, dim=1)  # => [B, n_steps, outC, H, W]
        return preds.permute(0,2,1,3,4)

################################################################################
# Example usage
################################################################################

if __name__ == "__main__":
    model_config = {
        'patch_size': 4,
        'pre_seq': 2,
        'dim': 256,
        'in_channels': 3,
        'out_channels': 2,
        'heads': 8,
        'dim_head': 32,
        'dropout': 0.1,
        'attn_dropout': 0.0,
        'drop_path': 0.25,
        'scale_dim': 4,
        'Ndepth': 6,
        'depth': 2,  # Might be unused in this code
    }
    model = PredFormer_Model(model_config, in_steps=2).to("cuda")
    print(model)

    # Example dummy input [B=2, T=4, C=3, H=64, W=64]
    x = torch.randn(64, 2, 2, 64, 64).to("cuda")
    inv = torch.randn(64, 11, 1, 64, 64).to("cuda")
    with torch.no_grad():
        y = model(x, inv, n_steps=10)
    print("Output shape:", y.shape)  # [2, 4, 3, 64, 64]
